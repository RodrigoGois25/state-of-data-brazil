{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ccc3d3-e32b-4df3-a528-6b0dde93d335",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccabcd-34d2-4e65-8176-2cb300b29d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0eaee-9946-4f01-9acd-0f5f2d0868e4",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfc8f5-e0ad-48b0-8db3-28dacfd6b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"data/sods.csv\")\n",
    "\n",
    "# Show Info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05134b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in 'cargo' column\n",
    "df = df.dropna(subset=[\"cargo\"])\n",
    "\n",
    "# Show NaN values in 'cargo' column\n",
    "df[\"cargo\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7eb70",
   "metadata": {},
   "source": [
    "# Pre-process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd72a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for Pre-processing\n",
    "pp_df = df.copy()\n",
    "\n",
    "# Show Info\n",
    "pp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa578fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the NaN values of the DataFrame\n",
    "pp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN in 'idade' with median and convert to int\n",
    "pp_df[\"idade\"] = pp_df[\"idade\"].fillna(pp_df[\"idade\"].median()).astype(int)\n",
    "pp_df[\"idade\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84642a0f-247f-4b0c-871c-6460ab20fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN in 'genero' with mode\n",
    "pp_df[\"genero\"] = pp_df[\"genero\"].fillna(pp_df[\"genero\"].mode()[0])\n",
    "pp_df[\"genero\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84056080-ef1e-4e4a-ab34-ca5905fe58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN in 'formacao' with 'nenhuma' value\n",
    "pp_df[\"formacao\"] = pp_df[\"formacao\"].fillna(\"nenhuma\")\n",
    "pp_df[\"formacao\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeca43c-1001-4df7-840b-6ee17f6f29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with too many NaN values\n",
    "pp_df = pp_df.drop(columns=[\"estado_moradia\", \"bancos_de_dados\"])\n",
    "\n",
    "# Drop rows with NaN values\n",
    "pp_df = pp_df.dropna()\n",
    "\n",
    "# Show the NaN values of the DataFrame\n",
    "pp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cffd3f-c09d-49b0-9697-87b010df81ed",
   "metadata": {},
   "source": [
    "# Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174ba4f-1066-41cc-b887-bb9bc21d3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for Transformation\n",
    "tt_df = pp_df.copy()\n",
    "\n",
    "# Show Info\n",
    "tt_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0f72e-a9d7-4169-b915-753526e14ff3",
   "metadata": {},
   "source": [
    "## Normalize Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize columns to lower case, strip whitespace, and remove accents\n",
    "def normalize_column(column):\n",
    "    return (\n",
    "        column.str.lower()\n",
    "        .str.strip()\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ascii\", errors=\"ignore\")\n",
    "        .str.decode(\"utf-8\")\n",
    "    )\n",
    "\n",
    "\n",
    "tt_df.iloc[:, 1:4] = tt_df.iloc[:, 1:4].apply(normalize_column)\n",
    "tt_df.iloc[:, 5:] = tt_df.iloc[:, 5:].apply(normalize_column)\n",
    "\n",
    "tt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9483346-d0bb-4840-8fd5-2b6b75c9780b",
   "metadata": {},
   "source": [
    "## \"nivel_ensino\" Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041136ce-d86e-47f6-b7ed-9278d5486e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'nivel_ensino' values to integers\n",
    "nivel_ensino_map = {\n",
    "    \"estudante de graduacao\": 1,\n",
    "    \"graduacao/bacharelado\": 2,\n",
    "    \"pos-graduacao\": 3,\n",
    "    \"mestrado\": 4,\n",
    "    \"doutorado ou phd\": 5,\n",
    "    \"nao tenho graduacao formal\": 0,\n",
    "    \"prefiro nao informar\": 0,\n",
    "}\n",
    "\n",
    "tt_df[\"nivel_ensino\"] = tt_df[\"nivel_ensino\"].replace(nivel_ensino_map)\n",
    "tt_df[\"nivel_ensino\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f09f4-344d-4278-a636-221a577eaf56",
   "metadata": {},
   "source": [
    "## \"tempo_experiencia_dados\" Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebd5ee-af01-4e11-aa47-68eea387309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'tempo_experiencia_dados' values to floating-point numbers\n",
    "tempo_experiencia_dados_map = {\n",
    "    \"menos de 1 ano\": 0.5,\n",
    "    \"de 1 a 2 anos\": 1.5,\n",
    "    \"de 3 a 4 anos\": 3.5,\n",
    "    \"de 4 a 6 anos\": 5.0,\n",
    "    \"de 7 a 10 anos\": 8.0,\n",
    "    \"mais de 10 anos\": 12.0,\n",
    "    \"nao tenho experiencia na area de dados\": 0,\n",
    "}\n",
    "\n",
    "tt_df[\"tempo_experiencia_dados\"] = tt_df[\"tempo_experiencia_dados\"].replace(\n",
    "    tempo_experiencia_dados_map\n",
    ")\n",
    "tt_df[\"tempo_experiencia_dados\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fdd3a-0045-4729-b080-ec21227b0534",
   "metadata": {},
   "source": [
    "## \"linguagens_preferidas\" Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0f894-6d37-4f8a-8db8-68a43b1f3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'linguagens_preferidas' values to a common set\n",
    "linguagens_preferidas_map = {\n",
    "    \"pyspark\": \"spark\",\n",
    "    \"xlsx\": \"excel\",\n",
    "    \"m language\": \"m\",\n",
    "    \"aql\": \"sql\",\n",
    "    \"sql postegres\": \"sql\",\n",
    "    \"nao sei\": \"nenhuma\",\n",
    "    \"nao utilizo\": \"nenhuma\",\n",
    "    \"softwares estatisticos como spss\": \"nenhuma\",\n",
    "    \"nao atuo com programacao\": \"nenhuma\",\n",
    "    \"nenhum\": \"nenhuma\",\n",
    "    \"nao uso\": \"nenhuma\",\n",
    "}\n",
    "tt_df[\"linguagens_preferidas\"] = tt_df[\"linguagens_preferidas\"].replace(\n",
    "    linguagens_preferidas_map\n",
    ")\n",
    "\n",
    "lp_min_freq = 5\n",
    "lp_counts = tt_df[\"linguagens_preferidas\"].explode().value_counts()\n",
    "common_lp = lp_counts[lp_counts >= lp_min_freq].index\n",
    "\n",
    "tt_df[\"linguagens_preferidas\"] = tt_df[\"linguagens_preferidas\"].apply(\n",
    "    lambda x: x if x in common_lp else \"outro\"\n",
    ")\n",
    "tt_df[\"linguagens_preferidas\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec4b8d",
   "metadata": {},
   "source": [
    "## \"bancos_de_dados\" Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'bancos_de_dados' into a list of lowercased values\n",
    "tt_df[\"bancos_de_dados_split\"] = tt_df[\"bancos_de_dados\"].str.lower().str.split(\",\")\n",
    "tt_df[\"bancos_de_dados_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f76036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "contagem = Counter()\n",
    "for lista in tt_df[\"bancos_de_dados_split\"]:\n",
    "    contagem.update([item.strip() for item in lista if item.strip() != \"\"])\n",
    "\n",
    "# Ordenar por frequÃªncia\n",
    "top_bancos = pd.Series(contagem).sort_values(ascending=False)\n",
    "print(top_bancos.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaade61",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "bancos_top = set(top_bancos.head(top_n).index)\n",
    "\n",
    "\n",
    "def simplificar(lista):\n",
    "    return [\n",
    "        item.strip() if item.strip() in bancos_top else \"outros\"\n",
    "        for item in lista\n",
    "        if item.strip() != \"\"\n",
    "    ]\n",
    "\n",
    "\n",
    "tt_df[\"bancos_de_dados_simplificado\"] = tt_df[\"bancos_de_dados_split\"].apply(\n",
    "    simplificar\n",
    ")\n",
    "tt_df[\"bancos_de_dados_simplificado\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "bds_encoded = mlb.fit_transform(tt_df[\"bancos_de_dados_simplificado\"])\n",
    "\n",
    "bds_df = pd.DataFrame(\n",
    "    bds_encoded,\n",
    "    columns=[f\"bds_{c.replace(' ', '_')}\" for c in mlb.classes_],\n",
    "    index=tt_df.index,\n",
    ")\n",
    "\n",
    "tt_df = pd.concat([tt_df, bds_df], axis=1).drop(\n",
    "    columns=[\"bancos_de_dados\", \"bancos_de_dados_split\", \"bancos_de_dados_simplificado\"]\n",
    ")\n",
    "tt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d2668-ce6d-43df-ad76-152520f319fd",
   "metadata": {},
   "source": [
    "## One Hot Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e13bb0-e359-44d5-82f2-4b8d1fe21479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_columns = tt_df.select_dtypes(include=[\"object\", \"bool\"]).columns.drop(\n",
    "    \"cargo\"\n",
    ")\n",
    "\n",
    "ohe_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded_data = ohe_encoder.fit_transform(tt_df[categorical_columns])\n",
    "new_columns = ohe_encoder.get_feature_names_out(categorical_columns)\n",
    "df_ohe = pd.DataFrame(encoded_data, columns=new_columns)\n",
    "\n",
    "df_not_phe = tt_df.drop(columns=categorical_columns).reset_index(drop=True)\n",
    "tt_df = pd.concat([df_not_phe, df_ohe], axis=1)\n",
    "\n",
    "tt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fdd6e-8f70-4f01-b940-1955c121a1a1",
   "metadata": {},
   "source": [
    "## Label Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c86b2-bb2f-40a2-8031-c3271ffd2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_label = label_encoder.fit_transform(tt_df[\"cargo\"])\n",
    "tt_df[\"cargo_label_encoded\"] = encoded_label\n",
    "tt_df[\"cargo_label_encoded\"][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50bf19-a770-42ac-b058-07e068a7e98e",
   "metadata": {},
   "source": [
    "## Drop \"cargo\" column and reset DataFrame index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9606723-d5b5-4a4d-b15e-f9b8b2d08cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df = tt_df.drop(columns=[\"cargo\"]).reset_index(drop=True)\n",
    "tt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f1e9f-d5f0-4acf-b9e9-f0b12f31d7df",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564432d-9fb5-4a1a-a020-17e8c97c6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tt_df.drop(columns=[\"cargo_label_encoded\"])\n",
    "y = tt_df[\"cargo_label_encoded\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape:{X_train.shape}\")\n",
    "print(f\"y_train shape:{y_train.shape}\")\n",
    "print(f\"X_test shape:{X_test.shape}\")\n",
    "print(f\"y_test shape:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b17e67-85ea-4a46-ae8a-86d16d9c179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "\n",
    "def run_experiment(model, param_grid, experiment_name=\"ML_Models\"):\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model, param_grid, cv=3, n_iter=50, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run(run_name=model.__class__.__name__):\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "        # PrediÃ§Ãµes no treino\n",
    "        y_train_pred = search.predict(X_train)\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision = precision_score(\n",
    "            y_train, y_train_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        train_recall = recall_score(\n",
    "            y_train, y_train_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # PrediÃ§Ãµes no teste\n",
    "        y_test_pred = search.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision = precision_score(\n",
    "            y_test, y_test_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        test_recall = recall_score(\n",
    "            y_test, y_test_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "        # Logar melhores hiperparÃ¢metros\n",
    "        mlflow.log_params(search.best_params_)\n",
    "\n",
    "        # Logar mÃ©tricas de treino\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "        mlflow.log_metric(\"train_precision\", train_precision)\n",
    "        mlflow.log_metric(\"train_recall\", train_recall)\n",
    "        mlflow.log_metric(\"train_f1\", train_f1)\n",
    "\n",
    "        # Logar mÃ©tricas de teste\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1\", test_f1)\n",
    "\n",
    "        # Salvar modelo treinado\n",
    "        mlflow.sklearn.log_model(search.best_estimator_, name=model.__class__.__name__)\n",
    "\n",
    "        print(\n",
    "            f\"{model.__class__.__name__} â Best Params: {search.best_params_}, \"\n",
    "            f\"Train Acc={train_accuracy:.4f}, Test Acc={test_accuracy:.4f}, \"\n",
    "            f\"Test Precision={test_precision:.4f}, Test Recall={test_recall:.4f}, Test F1={test_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# 1. Logistic Regression\n",
    "param_lr = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    \"max_iter\": [300, 500],\n",
    "}\n",
    "run_experiment(\n",
    "    LogisticRegression(random_state=42),\n",
    "    param_lr,\n",
    "    experiment_name=\"LogisticRegression\",\n",
    ")\n",
    "\n",
    "# 2. Decision Tree\n",
    "param_dt = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [2, 5, 10],\n",
    "}\n",
    "run_experiment(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_dt,\n",
    "    experiment_name=\"DecisionTree\",\n",
    ")\n",
    "\n",
    "# 3. Random Forest\n",
    "param_rf = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "}\n",
    "run_experiment(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_rf,\n",
    "    experiment_name=\"RandomForest\",\n",
    ")\n",
    "\n",
    "# 4. Support Vector Machine (SVM)\n",
    "param_svm = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "run_experiment(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_svm,\n",
    "    experiment_name=\"SVM\",\n",
    ")\n",
    "\n",
    "# 5. Rede Neural (MLPClassifier)\n",
    "param_mlp = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (100, 50)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"solver\": [\"adam\", \"lbfgs\"],\n",
    "    \"learning_rate_init\": [0.001, 0.01],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"max_iter\": [300, 500],\n",
    "}\n",
    "run_experiment(\n",
    "    MLPClassifier(random_state=42),\n",
    "    param_mlp,\n",
    "    experiment_name=\"MLPClassifier\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
